{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.learning_rate = 0.0001\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y, epochs=10000):\n",
    "        # adding a column of ones to the input matrix\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "            gradient = np.dot(X.T, (y - y_pred))\n",
    "            self.weights += self.learning_rate * gradient\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "        return np.round(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "        return np.column_stack([1 - y_pred, y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing of a dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df=pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "# df=df.drop(columns=[\"customerID\"],axis=1)\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# features = df.drop([\"Churn\"], axis=1)\n",
    "# target = df[\"Churn\"]\n",
    "\n",
    "# # null values\n",
    "# features.isnull().sum()\n",
    "# # duplicate values\n",
    "# features.duplicated().sum()\n",
    "# # replacing space in float type columns\n",
    "# features[\"TotalCharges\"]=features[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "\n",
    "# # features.isnull().sum()\n",
    "# features[\"TotalCharges\"]=features[\"TotalCharges\"].astype(float)\n",
    "# features.fillna(features[\"TotalCharges\"].mean(),inplace=True)\n",
    "# # Label Encoding the target variable\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# target = le.fit_transform(target)\n",
    "# #  converting object datatype to category\n",
    "# categorical_columns=[]\n",
    "# for col in features.columns:\n",
    "#     if features[col].dtype == 'object':\n",
    "#         categorical_columns.append(col)\n",
    "\n",
    "# # converting object datatype to category\n",
    "# for col in categorical_columns:\n",
    "#     features[col] = features[col].astype('category')\n",
    "\n",
    "# filter_col=list(features.select_dtypes(exclude=['category']).columns)\n",
    "\n",
    "# # one_hot encoding\n",
    "# features = pd.get_dummies(features)\n",
    "\n",
    "# # scaling the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# for col in filter_col:\n",
    "#     features[col] = scaler.fit_transform(features[[col]])\n",
    "\n",
    "# # features.head(10)\n",
    "\n",
    "# # target.shape\n",
    "\n",
    "# # Bagging\n",
    "# # ## Importing Libraries\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # ## Splitting the data, 80% for training and 20% for testing, among the training data, 80% for training and 20% for validation\n",
    "# x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "# x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FILE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Defining column names\n",
    "# columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "\n",
    "# # Reading the train and test datasets\n",
    "# adult_train = pd.read_csv(\"adult.data\", names=columns, skipinitialspace=True)\n",
    "# adult_test = pd.read_csv(\"adult.test\", names=columns, skiprows=1, skipinitialspace=True)\n",
    "# print(adult_train.shape, adult_test.shape)\n",
    "\n",
    "# # Now combining the train and test data\n",
    "# adult = pd.concat([adult_train, adult_test], axis=0)\n",
    "\n",
    "# # Replacing '?' with np.nan\n",
    "# adult.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# # Removing periods from 'income' column and converting it to binary\n",
    "# adult['income'] = adult['income'].apply(lambda x: x.replace(\".\", \"\"))\n",
    "# adult['income'] = adult['income'].map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "# # Replacing null values with the mode\n",
    "# for col in adult.columns:\n",
    "#     adult[col].fillna(adult[col].mode()[0], inplace=True)\n",
    "    \n",
    "# # Dropping duplicate values\n",
    "# adult.drop_duplicates(inplace=True)\n",
    "\n",
    "# # Separating features and target\n",
    "# features = adult.drop([\"income\"], axis=1)\n",
    "# target = adult[\"income\"]\n",
    "\n",
    "# # Converting object datatype columns to category\n",
    "# categorical_columns = []\n",
    "# for col in features.columns:\n",
    "#     if features[col].dtype == 'object':\n",
    "#         categorical_columns.append(col)\n",
    "    \n",
    "# for col in categorical_columns:\n",
    "#     features[col] = features[col].astype('category')\n",
    "    \n",
    "# # One-hot encoding categorical variables\n",
    "# features = pd.get_dummies(features)\n",
    "\n",
    "# # Computing correlation of each feature with the target\n",
    "# corr_matrix = features.corrwith(target).abs()\n",
    "\n",
    "# # Selecting the top 20 features based on the absolute correlation\n",
    "# top_20_features = corr_matrix.nlargest(20).index\n",
    "\n",
    "# # Keeping only the top 20 features\n",
    "# features = features[top_20_features]\n",
    "\n",
    "# # Scaling the top 20 features\n",
    "# scaler = StandardScaler()\n",
    "# features[top_20_features] = scaler.fit_transform(features[top_20_features])\n",
    "\n",
    "# # Splitting the data\n",
    "# x_train = features.iloc[:32561]\n",
    "# y_train = target.iloc[:32561]\n",
    "# x_test = features.iloc[32561:]\n",
    "# y_test = target.iloc[32561:]\n",
    "\n",
    "# # Creating validation data from the training data, 80% for training and 20% for validation\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# credit_data = pd.read_csv(\"creditcard.csv\")\n",
    "# credit_data.drop_duplicates(inplace=True)\n",
    "# # keeping all the positive class instances and randomly selecting 20000 negative class instances\n",
    "# positive_class = credit_data[credit_data[\"Class\"] == 1]\n",
    "# negative_class = credit_data[credit_data[\"Class\"] == 0]\n",
    "# negative_class = resample(negative_class, n_samples=20000, random_state=42)\n",
    "# credit_data = pd.concat([positive_class, negative_class], axis=0)\n",
    "\n",
    "# credit_data.drop_duplicates(inplace=True)\n",
    "# features = credit_data.drop([\"Class\"], axis=1)\n",
    "# target = credit_data[\"Class\"]\n",
    "\n",
    "# # scaling the data\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "\n",
    "# # splitting the data\n",
    "# x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = pd.DataFrame(features)  \n",
    "target = pd.Series(target)         \n",
    "\n",
    "\n",
    "models=[]\n",
    "accuracies=[]\n",
    "np.random.seed(42)\n",
    "for i in range(9):\n",
    "    indices = resample(range(len(x_train)), replace=True, n_samples=int( len(x_train)))\n",
    "    \n",
    "\n",
    "    sampled_features = features.iloc[indices]\n",
    "    sampled_target = target.iloc[indices]\n",
    "\n",
    "\n",
    "    model = logistic_regression()\n",
    "    model.fit(sampled_features, sampled_target)\n",
    "    models.append(model)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model on validation data, then adding those data as features to the validation data,so that we can train the meta model on it\n",
    "val_features = []\n",
    "\n",
    "for model in models:\n",
    "    val_features.append(model.predict(x_val))\n",
    "val_features = np.array(val_features).T\n",
    "\n",
    "# now adding the features to the validation data\n",
    "# x_val = np.concatenate((x_val, val_features), axis=1)\n",
    "xx_val =np.concatenate((x_val, val_features), axis=1)\n",
    "\n",
    "# training the meta model\n",
    "meta_model = logistic_regression()\n",
    "# meta_model.fit(x_val, y_val)\n",
    "meta_model.fit(xx_val, y_val)\n",
    "\n",
    "# running the model on the test data\n",
    "test_features = []\n",
    "for model in models:\n",
    "    test_features.append(model.predict(x_test))\n",
    "test_features = np.array(test_features).T\n",
    "\n",
    "# now adding the features to the test data\n",
    "# x_test = np.concatenate((x_test, test_features), axis=1)\n",
    "xx_test = np.concatenate((x_test, test_features), axis=1)\n",
    "\n",
    "# testing the meta model\n",
    "# y_pred = meta_model.predict(x_test)\n",
    "y_pred = meta_model.predict(xx_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance evaluating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute all performance metrics\n",
    "def evaluate_performance(y_true, y_pred, average=\"binary\"):\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # Sensitivity (Recall)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # AUROC\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auroc = None  # Handle cases where AUROC cannot be calculated\n",
    "    \n",
    "    # AUPR (Average Precision)\n",
    "    aupr = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion Matrix to calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    if auroc:\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(f\"AUPR: {aupr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"auroc\": auroc,\n",
    "        \"aupr\": aupr\n",
    "    }\n",
    "\n",
    "# metrics = evaluate_performance(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Collecting predictions from the 9 base models\n",
    "all_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.predict(x_test)  \n",
    "    all_predictions.append(y_pred)\n",
    "    \n",
    "# Transposing the predictions to get predictions for each instance\n",
    "all_predictions = np.array(all_predictions).T  # Shape: (n_samples, 9)\n",
    "\n",
    "# Performing majority voting (for classification)\n",
    "y_pred_voting = stats.mode(all_predictions, axis=1)[0].flatten()\n",
    "\n",
    "# Calculating accuracy for the voting ensemble\n",
    "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "print(\"Voting Ensemble Accuracy: \", voting_accuracy)\n",
    "metrics_voting = evaluate_performance(y_test, y_pred_voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Collecting predictions from the 9 base models on the test set\n",
    "test_features = []\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.predict(x_test)  # Predict using each base model\n",
    "    test_features.append(y_pred)\n",
    "    \n",
    "# Transpose the predictions to get predictions for each instance\n",
    "test_features = np.array(test_features).T  # Shape: (n_samples, 9)\n",
    "\n",
    "# Concatenate the original test features with the base models' predictions\n",
    "x_test_extended = np.concatenate((x_test, test_features), axis=1)\n",
    "\n",
    "# Use the meta-model to predict the final output\n",
    "y_pred_stacking = meta_model.predict(x_test_extended)  # Meta-model prediction\n",
    "\n",
    "# Calculate accuracy for the stacking ensemble\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "print(\"Stacking Ensemble Accuracy: \", stacking_accuracy)\n",
    "\n",
    "# Evaluate performance for stacking ensemble (optional metrics like precision, recall, etc.)\n",
    "metrics_stacking = evaluate_performance(y_test, y_pred_stacking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR along with violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Metric': [],\n",
    "    'Score': []\n",
    "}\n",
    "\n",
    "mean_std_metrics = {\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Sensitivity': [],\n",
    "    'Specificity': [],\n",
    "    'F1 Score': [],\n",
    "    'AUROC': [],\n",
    "    'AUPR': []\n",
    "}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred_base = model.predict(x_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_base)\n",
    "    precision = precision_score(y_test, y_pred_base)\n",
    "    sensitivity = recall_score(y_test, y_pred_base) \n",
    "    # specificity = recall_score(y_test, 1 - y_pred_base) \n",
    "    # # Specificity calculation\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_base).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(y_test, y_pred_base)\n",
    "    auroc = roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "    aupr = average_precision_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "\n",
    "    \n",
    "    metrics['Model'].extend([f'Model {i+1}'] * 7)  # 7 metrics for each model\n",
    "    metrics['Metric'].extend(['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1 Score', 'AUROC', 'AUPR'])\n",
    "    metrics['Score'].extend([accuracy, precision, sensitivity, specificity, f1, auroc, aupr])\n",
    "\n",
    "    # Appending to lists for mean and std dev calculation\n",
    "    mean_std_metrics['Accuracy'].append(accuracy)\n",
    "    mean_std_metrics['Precision'].append(precision)\n",
    "    mean_std_metrics['Sensitivity'].append(sensitivity)\n",
    "    mean_std_metrics['Specificity'].append(specificity)\n",
    "    mean_std_metrics['F1 Score'].append(f1)\n",
    "    mean_std_metrics['AUROC'].append(auroc)\n",
    "    mean_std_metrics['AUPR'].append(aupr)\n",
    "\n",
    "# Creating a DataFrame in long format\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Computing mean and standard deviation for each metric\n",
    "metric_stats = {metric: {\"mean\": np.mean(values), \"std\": np.std(values)} for metric, values in mean_std_metrics.items()}\n",
    "\n",
    "# Printing out mean and standard deviation for each metric\n",
    "print(\"\\nMean and Standard Deviation of Metrics:\")\n",
    "for metric, stats in metric_stats.items():\n",
    "    print(f\"{metric}: Mean = {stats['mean']:.4f}, Std Dev = {stats['std']:.4f}\")\n",
    "\n",
    "# Creating a violin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.color_palette(\"Paired\")\n",
    "sns.violinplot(x='Metric', y='Score', data=metrics_df, inner='quartile',palette=\"Paired\")\n",
    "plt.title('Violin Plot of Model Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# Annotating the plot with mean and std dev\n",
    "for i, metric in enumerate(mean_std_metrics.keys()):\n",
    "    mean_val = metric_stats[metric]['mean']\n",
    "    std_val = metric_stats[metric]['std']\n",
    "    # plt.text(i, mean_val, f'Mean: {mean_val:.4f}\\nStd Dev: {std_val:.4f}', \n",
    "    #          horizontalalignment='center', size='medium', color='black', weight='semibold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Displaying the metrics DataFrame\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.learning_rate = 0.0001\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y, epochs=10000):\n",
    "        # adding a column of ones to the input matrix\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "            gradient = np.dot(X.T, (y - y_pred))\n",
    "            self.weights += self.learning_rate * gradient\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "        return np.round(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1124, 45)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing of a dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"data1.csv\")\n",
    "df=df.drop(columns=[\"customerID\"],axis=1)\n",
    "df.drop_duplicates(inplace=True)\n",
    "features = df.drop([\"Churn\"], axis=1)\n",
    "target = df[\"Churn\"]\n",
    "\n",
    "# null values\n",
    "features.isnull().sum()\n",
    "# duplicate values\n",
    "features.duplicated().sum()\n",
    "# replacing space in float type columns\n",
    "features[\"TotalCharges\"]=features[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "\n",
    "# features.isnull().sum()\n",
    "features[\"TotalCharges\"]=features[\"TotalCharges\"].astype(float)\n",
    "features.fillna(features[\"TotalCharges\"].mean(),inplace=True)\n",
    "# Label Encoding the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(target)\n",
    "#  converting object datatype to category\n",
    "categorical_columns=[]\n",
    "for col in features.columns:\n",
    "    if features[col].dtype == 'object':\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "# converting object datatype to category\n",
    "for col in categorical_columns:\n",
    "    features[col] = features[col].astype('category')\n",
    "\n",
    "filter_col=list(features.select_dtypes(exclude=['category']).columns)\n",
    "\n",
    "# one_hot encoding\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for col in filter_col:\n",
    "    features[col] = scaler.fit_transform(features[[col]])\n",
    "\n",
    "# features.head(10)\n",
    "\n",
    "# target.shape\n",
    "\n",
    "# Bagging\n",
    "# ## Importing Libraries\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# ## Splitting the data, 80% for training and 20% for testing, among the training data, 80% for training and 20% for validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FILE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining column names\n",
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "\n",
    "# # Reading the train and test datasets\n",
    "# adult_train = pd.read_csv(\"adult.data\", names=columns, skipinitialspace=True)\n",
    "# adult_test = pd.read_csv(\"adult.test\", names=columns, skiprows=1, skipinitialspace=True)\n",
    "# print(adult_train.shape, adult_test.shape)\n",
    "# # Now combining the train and test data\n",
    "# adult = pd.concat([adult_train, adult_test], axis=0)\n",
    "\n",
    "\n",
    "# # replacing the '?' with np.nan\n",
    "# adult.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# # income column has a period at the end for some instances, removing it\n",
    "# adult['income'] = adult['income'].apply(lambda x: x.replace(\".\", \"\"))\n",
    "# # converting the income column to binary\n",
    "# adult['income'] = adult['income'].map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "# # replacing null values with the mode\n",
    "# for col in adult.columns:\n",
    "#     adult[col].fillna(adult[col].mode()[0], inplace=True)\n",
    "    \n",
    "# # dropping duplicate values\n",
    "# adult.drop_duplicates(inplace=True)\n",
    "\n",
    "# features = adult.drop([\"income\"], axis=1)\n",
    "# target = adult[\"income\"]\n",
    "\n",
    "# # converting object datatype to category\n",
    "# categorical_columns = []\n",
    "# for col in features.columns:\n",
    "#     if features[col].dtype == 'object':\n",
    "#         categorical_columns.append(col)\n",
    "    \n",
    "# for col in categorical_columns:\n",
    "#     features[col] = features[col].astype('category')\n",
    "    \n",
    "# # one-hot encoding\n",
    "# features = pd.get_dummies(features)\n",
    "\n",
    "# # scaling the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# filter_col = list(features.select_dtypes(exclude=['category']).columns)\n",
    "# for col in filter_col:\n",
    "#     features[col] = scaler.fit_transform(features[[col]])\n",
    "\n",
    "# # splitting the data\n",
    "# # if index is greater than 32561, then it is from the test data\n",
    "# features = pd.DataFrame(features)\n",
    "# target = pd.Series(target)\n",
    "# x_train = features.iloc[:32561]\n",
    "# y_train = target.iloc[:32561]\n",
    "# x_test = features.iloc[32561:]\n",
    "# y_test = target.iloc[32561:]\n",
    "\n",
    "# # now creating validation data from the training data, 80% for training and 20% for validation\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# credit_data = pd.read_csv(\"creditcard.csv\")\n",
    "# credit_data.drop_duplicates(inplace=True)\n",
    "# # keeping all the positive class instances and randomly selecting 20000 negative class instances\n",
    "# positive_class = credit_data[credit_data[\"Class\"] == 1]\n",
    "# negative_class = credit_data[credit_data[\"Class\"] == 0]\n",
    "# negative_class = resample(negative_class, n_samples=20000, random_state=42)\n",
    "# credit_data = pd.concat([positive_class, negative_class], axis=0)\n",
    "\n",
    "# credit_data.drop_duplicates(inplace=True)\n",
    "# features = credit_data.drop([\"Class\"], axis=1)\n",
    "# target = credit_data[\"Class\"]\n",
    "\n",
    "# # scaling the data\n",
    "# scaler = StandardScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "\n",
    "# # splitting the data\n",
    "# x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = pd.DataFrame(features)  \n",
    "target = pd.Series(target)         \n",
    "\n",
    "\n",
    "models=[]\n",
    "accuracies=[]\n",
    "\n",
    "for i in range(9):\n",
    "    indices = resample(range(len(x_train)), replace=True, n_samples=int(0.8 * len(x_train)))\n",
    "    \n",
    "\n",
    "    sampled_features = features.iloc[indices]\n",
    "    sampled_target = target.iloc[indices]\n",
    "\n",
    "\n",
    "    model = logistic_regression()\n",
    "    model.fit(x_train, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7914590747330961"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the model on validation data, then adding those data as features to the validation data,so that we can train the meta model on it\n",
    "val_features = []\n",
    "\n",
    "for model in models:\n",
    "    val_features.append(model.predict(x_val))\n",
    "val_features = np.array(val_features).T\n",
    "\n",
    "# now adding the features to the validation data\n",
    "# x_val = np.concatenate((x_val, val_features), axis=1)\n",
    "xx_val =np.concatenate((x_val, val_features), axis=1)\n",
    "\n",
    "# training the meta model\n",
    "meta_model = logistic_regression()\n",
    "# meta_model.fit(x_val, y_val)\n",
    "meta_model.fit(xx_val, y_val)\n",
    "\n",
    "# running the model on the test data\n",
    "test_features = []\n",
    "for model in models:\n",
    "    test_features.append(model.predict(x_test))\n",
    "test_features = np.array(test_features).T\n",
    "\n",
    "# now adding the features to the test data\n",
    "# x_test = np.concatenate((x_test, test_features), axis=1)\n",
    "xx_test = np.concatenate((x_test, test_features), axis=1)\n",
    "\n",
    "# testing the meta model\n",
    "# y_pred = meta_model.predict(x_test)\n",
    "y_pred = meta_model.predict(xx_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7915\n",
      "Precision: 0.5845\n",
      "Sensitivity (Recall): 0.5795\n",
      "Specificity: 0.8623\n",
      "F1-Score: 0.5820\n",
      "AUROC: 0.7209\n",
      "AUPR: 0.4441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute all performance metrics\n",
    "def evaluate_performance(y_true, y_pred, average=\"binary\"):\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # Sensitivity (Recall)\n",
    "    sensitivity = recall_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    \n",
    "    # AUROC\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auroc = None  # Handle cases where AUROC cannot be calculated\n",
    "    \n",
    "    # AUPR (Average Precision)\n",
    "    aupr = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion Matrix to calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    if auroc:\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(f\"AUPR: {aupr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"f1\": f1,\n",
    "        \"auroc\": auroc,\n",
    "        \"aupr\": aupr\n",
    "    }\n",
    "\n",
    "metrics = evaluate_performance(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble Accuracy:  0.795017793594306\n",
      "Accuracy: 0.7950\n",
      "Precision: 0.6026\n",
      "Sensitivity (Recall): 0.5341\n",
      "Specificity: 0.8822\n",
      "F1-Score: 0.5663\n",
      "AUROC: 0.7082\n",
      "AUPR: 0.4385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Collecting predictions from the 9 base models\n",
    "all_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.predict(x_test)  \n",
    "    all_predictions.append(y_pred)\n",
    "    \n",
    "# Transposing the predictions to get predictions for each instance\n",
    "all_predictions = np.array(all_predictions).T  # Shape: (n_samples, 9)\n",
    "\n",
    "# Performing majority voting (for classification)\n",
    "y_pred_voting = stats.mode(all_predictions, axis=1)[0].flatten()\n",
    "\n",
    "# Calculating accuracy for the voting ensemble\n",
    "voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "print(\"Voting Ensemble Accuracy: \", voting_accuracy)\n",
    "metrics_voting = evaluate_performance(y_test, y_pred_voting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Accuracy:  0.7914590747330961\n",
      "Accuracy: 0.7915\n",
      "Precision: 0.5845\n",
      "Sensitivity (Recall): 0.5795\n",
      "Specificity: 0.8623\n",
      "F1-Score: 0.5820\n",
      "AUROC: 0.7209\n",
      "AUPR: 0.4441\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Collect predictions from the 9 base models on the test set\n",
    "test_features = []\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.predict(x_test)  # Predict using each base model\n",
    "    test_features.append(y_pred)\n",
    "    \n",
    "# Step 2: Transpose the predictions to get predictions for each instance\n",
    "test_features = np.array(test_features).T  # Shape: (n_samples, 9)\n",
    "\n",
    "# Step 3: Concatenate the original test features with the base models' predictions\n",
    "x_test_extended = np.concatenate((x_test, test_features), axis=1)\n",
    "\n",
    "# Step 4: Use the meta-model to predict the final output\n",
    "y_pred_stacking = meta_model.predict(x_test_extended)  # Meta-model prediction\n",
    "\n",
    "# Step 5: Calculate accuracy for the stacking ensemble\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "print(\"Stacking Ensemble Accuracy: \", stacking_accuracy)\n",
    "\n",
    "# Step 6: Evaluate performance for stacking ensemble (optional metrics like precision, recall, etc.)\n",
    "metrics_stacking = evaluate_performance(y_test, y_pred_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'logistic_regression' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m specificity \u001b[38;5;241m=\u001b[39m recall_score(y_test, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_pred_base)  \u001b[38;5;66;03m# Specificity calculation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred_base)\n\u001b[0;32m---> 19\u001b[0m auroc \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(x_test)[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     20\u001b[0m aupr \u001b[38;5;241m=\u001b[39m average_precision_score(y_test, model\u001b[38;5;241m.\u001b[39mpredict_proba(x_test)[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'logistic_regression' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Sensitivity': [],  # Recall\n",
    "    'Specificity': [],\n",
    "    'F1 Score': [],\n",
    "    'AUROC': [],\n",
    "    'AUPR': []\n",
    "}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    y_pred_base = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_base)\n",
    "    precision = precision_score(y_test, y_pred_base)\n",
    "    sensitivity = recall_score(y_test, y_pred_base)  # Recall is sensitivity\n",
    "    specificity = recall_score(y_test, 1 - y_pred_base)  # Specificity calculation\n",
    "    f1 = f1_score(y_test, y_pred_base)\n",
    "    auroc = roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "    aupr = average_precision_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "\n",
    "    metrics['Model'].append(f'Model {i+1}')\n",
    "    metrics['Accuracy'].append(accuracy)\n",
    "    metrics['Precision'].append(precision)\n",
    "    metrics['Sensitivity'].append(sensitivity)\n",
    "    metrics['Specificity'].append(specificity)\n",
    "    metrics['F1 Score'].append(f1)\n",
    "    metrics['AUROC'].append(auroc)\n",
    "    metrics['AUPR'].append(aupr)\n",
    "\n",
    "# Meta model metrics\n",
    "meta_accuracy = accuracy_score(y_test, y_pred)\n",
    "meta_precision = precision_score(y_test, y_pred)\n",
    "meta_sensitivity = recall_score(y_test, y_pred)\n",
    "meta_specificity = recall_score(y_test, 1 - y_pred)\n",
    "meta_f1 = f1_score(y_test, y_pred)\n",
    "meta_auroc = roc_auc_score(y_test, meta_model.predict_proba(xx_test)[:, 1])\n",
    "meta_aupr = average_precision_score(y_test, meta_model.predict_proba(xx_test)[:, 1])\n",
    "\n",
    "metrics['Model'].append('Meta Model')\n",
    "metrics['Accuracy'].append(meta_accuracy)\n",
    "metrics['Precision'].append(meta_precision)\n",
    "metrics['Sensitivity'].append(meta_sensitivity)\n",
    "metrics['Specificity'].append(meta_specificity)\n",
    "metrics['F1 Score'].append(meta_f1)\n",
    "metrics['AUROC'].append(meta_auroc)\n",
    "metrics['AUPR'].append(meta_aupr)\n",
    "\n",
    "# Create a DataFrame of metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Violin Plot for Model Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='Model', y='Accuracy', data=metrics_df)\n",
    "plt.title('Violin Plot of Model Accuracies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
